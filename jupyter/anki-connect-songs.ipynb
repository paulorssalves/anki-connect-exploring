{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# VER TODO-LIST em README.md\n",
                "import re, os, string, csv\n",
                "import pandas as pd\n",
                "import json, time, urllib.request\n",
                "from tools.tokenization import (get_examples, get_frequency,\n",
                "                    get_text, get_tokens)\n",
                "from requests.exceptions import MissingSchema"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from cltk import NLP\n",
                "from cltk.lemmatize.grc import GreekBackoffLemmatizer\n",
                "lemmatizer = GreekBackoffLemmatizer()\n",
                "\n",
                "from cltk.alphabet.text_normalization import cltk_normalize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tools.wordstruct import Word as WikWord\n",
                "from tools.scraper import Word as BibleWord\n",
                "from tools.scraper import get_link, get_entry_soup, get_word_data, fetch_group_as_string, BASE_URL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ways to perfect this function:\n",
                "# check whether the first word in the concordance example is it\n",
                "# if not, cut off this word (so as to remove incomplete words)\n",
                "# do the same to the last word of the sentence\n",
                "\n",
                "def boldify_selected_word(word, string):\n",
                "    string_l = string.split()\n",
                "    for index in range(len(string_l)):\n",
                "        if string_l[index].lower() == word.lower():\n",
                "            bold = \"<b>\"+word.lower()+\"</b>\"\n",
                "            string_l[index] = bold \n",
                "    return string_l, bold\n",
                "\n",
                "def get_context_and_clean_up(word, concordance):\n",
                "\n",
                "    # get word context from concordance\n",
                "    c = concordance.concordance_list(cltk_normalize(word))\n",
                "\n",
                "    # split string and make chosen word bold\n",
                "    line, bold_word = boldify_selected_word(cltk_normalize(word), c[0][6])\n",
                "    word_list = line\n",
                "\n",
                "    # remove first and last words of string, as they are sometimes incomplete\n",
                "    # and assert that the first and last \n",
                "    if word_list[0] not in (word, bold_word):\n",
                "        del word_list[0]\n",
                "        if word_list[0] in string.punctuation:\n",
                "            del word_list[0] \n",
                "    if word_list[len(word_list) - 1] not in (word, bold_word):\n",
                "        del word_list[len(word_list) - 1]\n",
                "\n",
                "    # get string with extra spaces\n",
                "    unfinished_string = \" \".join(word_list)\n",
                "    \n",
                "    # remove unnecessary spaces from string\n",
                "    trailing_spaces_start = r\"^\\s+\"\n",
                "    trailing_spaces_end = r\"\\s+$\"\n",
                "\n",
                "    cleaned_up_pre = re.sub(r'\\s([?.!,:;\"](?:\\s|$))', r'\\1', unfinished_string)\n",
                "    cleaned_up_start = re.sub(trailing_spaces_start, \"\", cleaned_up_pre)\n",
                "    final_string = re.sub(trailing_spaces_end, \"\", cleaned_up_start)\n",
                "    \n",
                "    return final_string\n",
                "\n",
                "def clean_up(string):\n",
                "    trailing_spaces_start = r\"^\\s+\"\n",
                "    trailing_spaces_end = r\"\\s+$\"\n",
                "\n",
                "    cleaned_up_pre = re.sub(r'([?.!,:;\"](?:\\s|$))', \"\", string.split()[0])\n",
                "    cleaned_up_start = re.sub(trailing_spaces_start, \"\", cleaned_up_pre)\n",
                "    cleaned_up = re.sub(trailing_spaces_end, \"\", cleaned_up_start)\n",
                "\n",
                "    return cleaned_up"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "œÑ·Ω∏\n"
                    ]
                }
            ],
            "source": [
                "a = \"Œ§·Ω∏\".lower()\n",
                "for word in wf.concordance_list(\"Œ§·Ω∏\")[0][6].split():\n",
                "    if word == a:\n",
                "        print(word)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚Äéê§Ä CLTK version '1.0.23'.\n",
                        "Pipeline for language 'Ancient Greek' (ISO: 'grc'): `GreekNormalizeProcess`, `GreekStanzaProcess`, `GreekEmbeddingsProcess`, `StopsProcess`, `GreekNERProcess`.\n"
                    ]
                }
            ],
            "source": [
                "songs, wf = get_text(\"textos/apolitikia.txt\")\n",
                "tokens, token_set, phrases = get_tokens(songs)\n",
                "frequency = get_frequency(tokens)\n",
                "cltk_nlp = NLP(language=\"grc\")\n",
                "cltk_doc = cltk_nlp.analyze(songs)\n",
                "\n",
                "wordlist = []\n",
                "for pair in frequency:\n",
                "    token = pair[0]\n",
                "    wordlist.append(token)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def request(action, **params):\n",
                "    return {'action': action,\n",
                "            'params': params,\n",
                "            'version': 6}\n",
                "\n",
                "def invoke(action, **params):\n",
                "    requestJson = json.dumps(request(action, **params)).encode('utf-8')\n",
                "    response = json.load(urllib.request.urlopen(urllib.request.Request('http://localhost:8765', requestJson)))\n",
                "\n",
                "    if len(response) != 2:\n",
                "        raise Exception('response has an unexpected number of fields')\n",
                "\n",
                "    if 'error' not in response:\n",
                "        raise Exception('response is missing required error field')\n",
                "    \n",
                "    if 'result' not in response:\n",
                "        raise Exception('response is missing required result field')\n",
                "\n",
                "    if response['error'] is not None:\n",
                "        raise Exception(response['error'])\n",
                "\n",
                "    return response['result']\n",
                "\n",
                "noteIDs = invoke('findNotes', query='deck:Grego') \n",
                "notesInfo = invoke('notesInfo', notes=noteIDs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_input_for_material(wordlist, anki_cards):\n",
                "\n",
                "    notes = []\n",
                "    for card in anki_cards:\n",
                "        notes.append(cltk_normalize(clean_up(card['fields']['Word']['value'])))\n",
                "    fwl = [cltk_normalize(word) for word in wordlist if word not in notes]\n",
                "    notes_group = lemmatizer.lemmatize(notes)\n",
                "    fwl_group = lemmatizer.lemmatize(fwl)\n",
                "    notes_lemmas = set([tuple[1] for tuple in notes_group])\n",
                "    fwl_lemmas = set([tuple[1] for tuple in fwl_group])\n",
                "\n",
                "    filtered_fwl_lemmas = [word for word in fwl_lemmas if word not in notes_lemmas]\n",
                "    reworked_fwl = [tuple for tuple in fwl_group if tuple[1] in filtered_fwl_lemmas]\n",
                "\n",
                "    for i in range(len(reworked_fwl)):\n",
                "        if reworked_fwl[i][0] == reworked_fwl[i][1]:\n",
                "            reworked_fwl[i] = reworked_fwl[i][0]\n",
                "\n",
                "    return reworked_fwl\n",
                "\n",
                "reworked_fwl = get_input_for_material(wordlist, notesInfo)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "reworked_fwl\n",
                "f = open(\"output/apolitikia.csv\", encoding=\"UTF8\")\n",
                "reader = csv.reader(f)\n",
                "for line in reader:\n",
                "    for entry in reworked_fwl:\n",
                "        if type(entry) == str:\n",
                "            if line[7] == entry:\n",
                "                del reworked_fwl[reworked_fwl.index(entry)]\n",
                "        elif type(entry) == tuple:\n",
                "            if line[7] in (entry[0], entry[1]):\n",
                "                del reworked_fwl[reworked_fwl.index(entry)]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def refilter_fwl(list: list, filename: str) -> list:\n",
                "    f = open(os.path.join(\"output\", filename+\".csv\"), encoding=\"UTF8\")\n",
                "    reader = csv.reader(f)\n",
                "    for line in reader:\n",
                "        for entry in list:\n",
                "            if type(entry) == str:\n",
                "                if line[7] == entry:\n",
                "                    del list[list.index(entry)]\n",
                "            elif type(entry) == tuple:\n",
                "                if line[7] in (list[0], list[1]):\n",
                "                    del list[list.index(entry)]\n",
                "\n",
                "reworked_fwl = refilter_fwl(reworked_fwl, \"apolitikia\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['œÑœÅŒπŒÆŒºŒµœÅŒøœÇ',\n",
                            " 'Œ¶ŒπŒªŒ¨ŒΩŒ∏œÅœâœÄŒµ',\n",
                            " 'Œï·ΩêœÜœÅŒ±ŒπŒΩŒ≠œÉŒ∏œâ',\n",
                            " ('Œø·ΩêœÅŒ¨ŒΩŒπŒ±', 'ŒøœÖÃìœÅŒ¨ŒΩŒπŒøœÇ'),\n",
                            " '·ºòœÄŒ¨œÑŒ∑œÉŒµ',\n",
                            " 'Œ§·Ω∏',\n",
                            " ('œÜŒ±ŒπŒ¥œÅœåŒΩ', 'œÜŒ±ŒπŒ¥œÅœåœÇ'),\n",
                            " 'œÑŒ∑œÇ',\n",
                            " '·ºàŒΩŒ±œÉœÑŒ¨œÉŒµœâœÇ',\n",
                            " 'Œ∫ŒÆœÅœÖŒ≥ŒºŒ±',\n",
                            " '·ºàŒ≥Œ≥Œ≠ŒªŒøœÖ',\n",
                            " ('ŒºŒ±Œ∏Œø·ø¶œÉŒ±Œπ', 'ŒºŒ±ŒΩŒ∏Œ¨ŒΩœâ'),\n",
                            " 'ŒöœÖœÅŒØŒøœÖ',\n",
                            " 'ŒúŒ±Œ∏ŒÆœÑœÅŒπŒ±Œπ',\n",
                            " 'œÄœÅŒøŒ≥ŒøŒΩŒπŒ∫·Ω¥ŒΩ',\n",
                            " ('·ºÄœÄœåœÜŒ±œÉŒπŒΩ', '·ºÄœÄœåœÜŒ±œÉŒπœÇ'),\n",
                            " ('·ºÄœÄŒøœÅœÅŒØœàŒ±œÉŒ±Œπ', '·ºÄœÄŒøœÅœÅŒØœÄœÑœâ'),\n",
                            " ('Œ∫Œ±œÖœáœéŒºŒµŒΩŒ±Œπ', 'Œ∫Œ±œÖœáŒ¨ŒøŒºŒ±Œπ'),\n",
                            " '·ºòœÉŒ∫œçŒªŒµœÖœÑŒ±Œπ',\n",
                            " 'ŒßœÅŒπœÉœÑ·Ω∏œÇ',\n",
                            " 'Œ§·Ω∏ŒΩ',\n",
                            " ('œÉœÖŒΩŒ¨ŒΩŒ±œÅœáŒøŒΩ', 'œÉœçŒΩ,·ºÄŒΩŒ¨·ºÑœÅœáœâ'),\n",
                            " 'ŒõœåŒ≥ŒøŒΩ',\n",
                            " 'Œ†Œ±œÑœÅ·Ω∂',\n",
                            " 'Œ†ŒΩŒµœçŒºŒ±œÑŒπ',\n",
                            " 'Œ†Œ±œÅŒ∏Œ≠ŒΩŒøœÖ',\n",
                            " ('·ºÄŒΩŒ±ŒºŒΩŒÆœÉœâŒºŒµŒΩ', '·ºÄŒΩŒ±ŒºŒπŒºŒΩŒÆœÉŒ∫œâ'),\n",
                            " ('Œ∑·ΩêŒ¥œåŒ∫Œ∑œÉŒµ', 'ŒµœÖÃìŒ¥ŒøŒ∫Œ≠œâ'),\n",
                            " ('·ΩëœÄŒøŒºŒµ·øñŒΩŒ±Œπ', '·ΩëœÄŒøŒºŒ≠ŒΩœâ'),\n",
                            " '·ºàŒ≥Œ≥ŒµŒªŒπŒ∫Œ±·Ω∂',\n",
                            " ('·ºÄœÄŒµŒΩŒµŒ∫œÅœéŒ∏Œ∑œÉŒ±ŒΩ', '·ºÄœÄœåŒΩŒµŒ∫œÅœåœâ'),\n",
                            " ('Œ∂Œ∑œÑŒø·ø¶œÉŒ±', 'Œ∂Œ∑œÑŒ≠œâ'),\n",
                            " '·ºòœÉŒ∫œçŒªŒµœÖœÉŒ±œÇ',\n",
                            " 'œÄŒµŒπœÅŒ±œÉŒ∏Œµ·Ω∂œÇ',\n",
                            " ('·ΩëœÄ', '·ΩëœÄœå'),\n",
                            " '·ΩôœÄŒÆŒΩœÑŒ∑œÉŒ±œÇ',\n",
                            " 'Œ†Œ±œÅŒ∏Œ≠ŒΩ·ø≥',\n",
                            " 'ŒöŒ±œÑŒ≠ŒªœÖœÉŒ±œÇ',\n",
                            " '·º†ŒΩŒ≠œâŒæŒ±œÇ',\n",
                            " 'ŒõŒ∑œÉœÑ·øá',\n",
                            " 'Œ†Œ±œÅŒ¨Œ¥ŒµŒπœÉŒøŒΩ',\n",
                            " 'ŒúœÖœÅŒøœÜœåœÅœâŒΩ',\n",
                            " ('ŒºŒµœÑŒ≠Œ≤Œ±ŒªŒµœÇ', 'ŒºŒµœÑŒ±Œ≤Œ¨ŒªŒªœâ'),\n",
                            " ('Œ∫Œ∑œÅœçœÑœÑŒµŒπŒΩ', 'Œ∫Œ∑œÅœçœÉœÉœâ'),\n",
                            " ('œÄŒ±œÅŒ≠œáœâŒΩ', 'œÄŒ±œÅŒ≠œáœâ'),\n",
                            " '·ºòŒæ',\n",
                            " ('·ΩïœàŒøœÖœÇ', '·ΩïœàŒøœÇ'),\n",
                            " ('Œµ·ΩîœÉœÄŒªŒ±Œ≥œáŒΩŒøœÇ', 'ŒµœÖÃìÃÅœÉœÄŒªŒ±Œ≥œáŒΩŒøœÇ'),\n",
                            " 'œÑŒ±œÜ·Ω¥ŒΩ',\n",
                            " ('Œ∫Œ±œÑŒµŒ¥Œ≠Œæœâ', 'Œ∫Œ±œÑŒ±Œ¥ŒµŒØŒ∫ŒΩœÖŒºŒπ'),\n",
                            " ('œÑœÅŒπŒÆŒºŒµœÅŒøŒΩ', 'œÑœÅŒπŒÆŒºŒµœÅŒøœÇ'),\n",
                            " '·ºêŒªŒµœÖŒ∏ŒµœÅœéœÉŒ∑œÇ',\n",
                            " ('œÄŒ±Œ∏·ø∂ŒΩ', 'œÄŒ¨Œ∏ŒøœÇ'),\n",
                            " '·ºàŒΩŒ¨œÉœÑŒ±œÉŒπœÇ',\n",
                            " '·ºÑœáœÅŒ±ŒΩœÑœåŒΩ',\n",
                            " 'Œ£·ø∂ŒºŒ±',\n",
                            " 'Œ£ŒøŒπ',\n",
                            " 'ŒñœâŒøŒ¥œåœÑŒ±',\n",
                            " '·ºàŒΩŒ±œÉœÑŒ¨œÉŒµŒπ',\n",
                            " '·ΩçœÑŒµ',\n",
                            " 'Œ∂œâ·Ω¥',\n",
                            " ('·ºÑŒ¥Œ∑ŒΩ', '·ºÖŒ¥Œ∑ŒΩ'),\n",
                            " ('œÑŒµŒ∏ŒΩŒµ·ø∂œÑŒ±œÇ', 'Œ∏ŒΩŒÆœÉŒ∫œâ'),\n",
                            " 'ŒßœÅŒπœÉœÑ·Ω≤',\n",
                            " '·ºàœÄŒøœÉœÑœåŒªŒøŒπœÇ',\n",
                            " 'Œ£œÑŒ±œÖœÅ·ø∑',\n",
                            " 'Œ£ŒøœÖ',\n",
                            " 'ŒîœÖŒΩŒ¨ŒºŒµŒπœÇ']"
                        ]
                    },
                    "execution_count": 41,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "reworked_fwl"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "def fetch_bible_word(word):\n",
                "    link = get_link(BASE_URL, word)\n",
                "    soup = get_entry_soup(link)\n",
                "    word = BibleWord(get_word_data(soup))\n",
                "\n",
                "    return word"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "def searcher(word):\n",
                "\n",
                "    if type(word) == tuple:\n",
                "        try:\n",
                "            bible_word = fetch_bible_word(word[0])\n",
                "            return {\"search_num\": 1, \"source\": \"BibleHub\", \"input\": word[0], \"output\": bible_word.data}\n",
                "\n",
                "        except MissingSchema:\n",
                "            try:\n",
                "                bible_word = fetch_bible_word(word[1])\n",
                "                return {\"search_num\": 2, \"source\": \"BibleHub\", \"input\": word[0], \"output\": bible_word.data}\n",
                "\n",
                "            except MissingSchema:\n",
                "\n",
                "                return {\"search_num\": 2, \"source\": None, \"input\": word[0], \"output\": None}\n",
                "                \n",
                "\n",
                "    elif type(word) == str:\n",
                "        try: \n",
                "            bible_word = fetch_bible_word(word)\n",
                "            return {\"search_num\": 1, \"source\": \"BibleHub\", \"input\": word, \"output\": bible_word.data}\n",
                "\n",
                "        except:\n",
                "                return {\"search_num\": 1, \"source\": None, \"input\": word, \"output\": None}\n",
                "\n",
                "    else:\n",
                "\n",
                "        return {\"search_num\": 0, \"source\": None, \"input\": word, \"output\": None}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "word no. 1: ·ΩâŒ¥Œø·Ω∂\n",
                        "word no. 2: ('Œ¥ŒπŒ±œÜŒøœÅ·Ω∞', 'Œ¥ŒπŒ±œÜŒøœÅŒ¨')\n",
                        "word no. 3: œÉŒµŒ±œÖœÑœåŒΩ¬∑\n",
                        "word no. 4: Œ≥ŒØŒΩŒµœÉŒ∏Œ±ŒØ\n",
                        "word no. 5: ('[', 'punc')\n",
                        "word no. 6: Œï·ΩêŒªŒøŒ≥Œµ·øñœÑŒµ\n",
                        "word no. 7: ·ΩëŒº·æ∂œÇ¬∑\n",
                        "word no. 8: Œü·Ωêœá·Ω∂\n",
                        "Wait... 0/60\n",
                        "Wait... 1/60\n",
                        "Wait... 2/60\n",
                        "Wait... 3/60\n",
                        "Wait... 4/60\n",
                        "Wait... 5/60\n",
                        "Wait... 6/60\n",
                        "Wait... 7/60\n",
                        "Wait... 8/60\n",
                        "Wait... 9/60\n",
                        "Wait... 10/60\n",
                        "Wait... 11/60\n",
                        "Wait... 12/60\n",
                        "Wait... 13/60\n",
                        "Wait... 14/60\n",
                        "Wait... 15/60\n",
                        "Wait... 16/60\n",
                        "Wait... 17/60\n",
                        "Wait... 18/60\n",
                        "Wait... 19/60\n",
                        "Wait... 20/60\n",
                        "Wait... 21/60\n",
                        "Wait... 22/60\n",
                        "Wait... 23/60\n",
                        "Wait... 24/60\n",
                        "Wait... 25/60\n",
                        "Wait... 26/60\n",
                        "Wait... 27/60\n",
                        "Wait... 28/60\n",
                        "Wait... 29/60\n",
                        "Wait... 30/60\n",
                        "Wait... 31/60\n",
                        "Wait... 32/60\n",
                        "Wait... 33/60\n",
                        "Wait... 34/60\n",
                        "Wait... 35/60\n",
                        "Wait... 36/60\n",
                        "Wait... 37/60\n",
                        "Wait... 38/60\n",
                        "Wait... 39/60\n",
                        "Wait... 40/60\n",
                        "Wait... 41/60\n",
                        "Wait... 42/60\n",
                        "Wait... 43/60\n",
                        "Wait... 44/60\n",
                        "Wait... 45/60\n",
                        "Wait... 46/60\n",
                        "Wait... 47/60\n",
                        "Wait... 48/60\n",
                        "Wait... 49/60\n",
                        "Wait... 50/60\n",
                        "Wait... 51/60\n",
                        "Wait... 52/60\n",
                        "Wait... 53/60\n",
                        "Wait... 54/60\n",
                        "Wait... 55/60\n",
                        "Wait... 56/60\n",
                        "Wait... 57/60\n",
                        "Wait... 58/60\n",
                        "Wait... 59/60\n"
                    ]
                }
            ],
            "source": [
                "# √â necess√°rio criar alguma forma de verificar se\n",
                "# as palavras que o script adquiriu correspondem √†s palavras\n",
                "# do input, uma vez que as palavras obtidas do BibleHub podem ser,\n",
                "# devido ao meu algoritmo meio porco, n√£o muito confi√°veis\n",
                "\n",
                "# uma ideia interessante seria adicionar um marcador ao lado, algo como \n",
                "\n",
                "# Mais uma coisa a aperfei√ßoar:\n",
                "# Se a mesma palavra j√° estiver presente, deixar de lado\n",
                "# n√£o o mesmo radical, mas *a mesma palavra*\n",
                "\n",
                "def acquire_data(list, amount=None):\n",
                "\n",
                "    bible_searches = 0\n",
                "    data = []\n",
                "    blanks = []\n",
                "    already_present = []\n",
                "\n",
                "    if (amount != None) and (amount <= len(list)):\n",
                "        number = range(amount)\n",
                "    else:\n",
                "        number = range(len(list))\n",
                "\n",
                "    for index in number:\n",
                "        print(\"word no. {}: {}\".format(index+1, list[index]))\n",
                "        word_dict = searcher(list[index])\n",
                "\n",
                "        if word_dict[\"source\"] != None:\n",
                "            result = (word_dict[\"source\"], word_dict[\"input\"], word_dict[\"output\"])\n",
                "            if result[2] not in already_present:\n",
                "                data.append(result)\n",
                "            already_present.append(result[2])\n",
                "        else:\n",
                "            blanks.append(word_dict[\"input\"])\n",
                "\n",
                "        bible_searches += word_dict[\"search_num\"]\n",
                "\n",
                "        if bible_searches >= 10:\n",
                "            for n in range(60):\n",
                "                time.sleep(1)\n",
                "                print(\"Wait... {}/60\".format(n))\n",
                "                bible_searches = 0\n",
                "    \n",
                "    return data, blanks\n",
                "\n",
                "data, blanks = acquire_data(reworked_fwl,8)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "·ΩâŒ¥Œø·Ω∂\n",
                        "Œï·ΩêŒªŒøŒ≥Œµ·øñœÑŒµ\n",
                        "Œü·Ωêœá·Ω∂\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "def produce_material(output_file_name, data=None, blanks=None):\n",
                "    if type(output_file_name) is not str:\n",
                "        raise TypeError(\"variable 'output_file_name' must be of type 'str'.\")\n",
                "\n",
                "    if (data is not None) and (data != []): \n",
                "        for item in data:\n",
                "            if item[0] is not None:\n",
                "\n",
                "                print(item[1])\n",
                "\n",
                "                curr = item[2]\n",
                "                concordances = curr[\"concordances\"]\n",
                "\n",
                "                dc = {\n",
                "                    \"word\": clean_up(concordances[\"Original Word\"]),\n",
                "                    \"phonetics\": concordances[\"Phonetic Spelling\"],\n",
                "                    \"category\": concordances[\"Part of Speech\"],\n",
                "                    \"meaning\": concordances['Definition'],\n",
                "                    \"greek\": fetch_group_as_string([tuple[0] for tuple in curr['examples']], single_list=True),\n",
                "                    \"english\": fetch_group_as_string([tuple[1] for tuple in curr['examples']], single_list=True)\n",
                "                }\n",
                "                dc[\"context\"] = \"\\\"\"+get_context_and_clean_up(item[1], wf)+\"\\\"\"\n",
                "                dc[\"original\"] = item[1]        \n",
                "                dc[\"source\"] = item[0]        \n",
                "                # to display which word is being worked upon at the time\n",
                "\n",
                "                words_dataframe = pd.DataFrame.from_dict(dc, orient=\"index\")\n",
                "                words_dataframe = words_dataframe.transpose()\n",
                "                words_dataframe.to_csv(os.path.join(\"output\", output_file_name+\".csv\"), \n",
                "                                                    encoding=\"utf-8\", mode=\"a\", \n",
                "                                                    header=False, index=False)\n",
                "            \n",
                "    if (blanks is not None) and (blanks != []):\n",
                "        blanks_dataframe = pd.DataFrame(blanks)\n",
                "        blanks_dataframe= blanks_dataframe.transpose()\n",
                "        blanks_dataframe.to_csv(os.path.join(\"blanks\", output_file_name+\"_blanks.csv\"), \n",
                "                                            encoding=\"utf-8\", mode=\"a\", \n",
                "                                            header=False, index=False)\n",
                "\n",
                "produce_material(\"test\", data=data, blanks=None)\n",
                "regex = re.compile(\"\\\"\\\"\")\n",
                "with open(os.path.join(\"output\", \"test.csv\")) as file:\n",
                "    for line in file:\n",
                "        regex.sub(\"\", line)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "word no. 1: ·ΩâŒ¥Œø·Ω∂\n",
                        "word no. 2: ('Œ¥ŒπŒ±œÜŒøœÅ·Ω∞', 'Œ¥ŒπŒ±œÜŒøœÅŒ¨')\n",
                        "word no. 3: œÉŒµŒ±œÖœÑœåŒΩ¬∑\n",
                        "word no. 4: Œ≥ŒØŒΩŒµœÉŒ∏Œ±ŒØ\n",
                        "word no. 5: ('[', 'punc')\n",
                        "word no. 6: Œï·ΩêŒªŒøŒ≥Œµ·øñœÑŒµ\n"
                    ]
                }
            ],
            "source": [
                "d = acquire_data(reworked_fwl[0:6])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "([('BibleHub',\n",
                            "   '·ΩâŒ¥Œø·Ω∂',\n",
                            "   {'concordances': {'Original Word': '·ΩÅŒ¥œåœÇ, Œø·ø¶, ·º°',\n",
                            "     'Part of Speech': 'Noun, Feminine',\n",
                            "     'Transliteration': 'hodos',\n",
                            "     'Phonetic Spelling': \"(hod-os')\",\n",
                            "     'Definition': 'a way, road',\n",
                            "     'Usage': 'a way, road, journey, path.'},\n",
                            "    'examples': [('·ºôœÑŒøŒπŒºŒ¨œÉŒ±œÑŒµ œÑ·Ω¥ŒΩ ·ΩÅŒ¥·Ω∏ŒΩ ŒöœÖœÅŒØŒøœÖ Œµ·ΩêŒ∏ŒµŒØŒ±œÇ',\n",
                            "      ' Prepare the way of [the] Lord straight'),\n",
                            "     ('Œ≥·øÜ ŒùŒµœÜŒ∏Œ±ŒªŒØŒº ·ΩÅŒ¥·Ω∏ŒΩ Œ∏Œ±ŒªŒ¨œÉœÉŒ∑œÇ œÄŒ≠œÅŒ±ŒΩ',\n",
                            "      ' land of Naphtali way of [the] sea beyond'),\n",
                            "     ('·ºêŒΩ œÑ·øá ·ΩÅŒ¥·ø∑ ŒºŒÆ œÄŒøœÑŒ≠', ' on the way lest ever')]}),\n",
                            "  ('BibleHub',\n",
                            "   'Œï·ΩêŒªŒøŒ≥Œµ·øñœÑŒµ',\n",
                            "   {'concordances': {'Original Word': 'Œµ·ΩêŒªŒøŒ≥Œ≠œâ',\n",
                            "     'Part of Speech': 'Verb',\n",
                            "     'Transliteration': 'euloge√≥',\n",
                            "     'Phonetic Spelling': \"(yoo-log-eh'-o)\",\n",
                            "     'Definition': 'to speak well of, praise',\n",
                            "     'Usage': '(lit: I speak well of) I bless; pass: I am blessed.'},\n",
                            "    'examples': [('œÑ·Ω∏ŒΩ Œø·ΩêœÅŒ±ŒΩ·Ω∏ŒΩ Œµ·ΩêŒªœåŒ≥Œ∑œÉŒµŒΩ Œ∫Œ±·Ω∂ Œ∫ŒªŒ¨œÉŒ±œÇ',\n",
                            "      ' heaven he blessed and having broken'),\n",
                            "     ('œÖ·º±·ø∑ ŒîŒ±œÖŒØŒ¥ Œï·ΩêŒªŒøŒ≥Œ∑ŒºŒ≠ŒΩŒøœÇ ·ΩÅ ·ºêœÅœáœåŒºŒµŒΩŒøœÇ',\n",
                            "      ' Son of David blessed [is] he who comes'),\n",
                            "     ('·ºÇŒΩ Œµ·º¥œÄŒ∑œÑŒµ Œï·ΩêŒªŒøŒ≥Œ∑ŒºŒ≠ŒΩŒøœÇ ·ΩÅ ·ºêœÅœáœåŒºŒµŒΩŒøœÇ',\n",
                            "      ' anyhow you say Blessed [is] he who comes')]})],\n",
                            " ['Œ¥ŒπŒ±œÜŒøœÅ·Ω∞', 'œÉŒµŒ±œÖœÑœåŒΩ¬∑', 'Œ≥ŒØŒΩŒµœÉŒ∏Œ±ŒØ', '['])"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "d"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "('BibleHub', 'Œï·ΩêŒªŒøŒ≥Œµ·øñœÑŒµ', {'concordances': {'Original Word': 'Œµ·ΩêŒªŒøŒ≥Œ≠œâ', 'Part of Speech': 'Verb', 'Transliteration': 'euloge√≥', 'Phonetic Spelling': \"(yoo-log-eh'-o)\", 'Definition': 'to speak well of, praise', 'Usage': '(lit: I speak well of) I bless; pass: I am blessed.'}, 'examples': [('œÑ·Ω∏ŒΩ Œø·ΩêœÅŒ±ŒΩ·Ω∏ŒΩ Œµ·ΩêŒªœåŒ≥Œ∑œÉŒµŒΩ Œ∫Œ±·Ω∂ Œ∫ŒªŒ¨œÉŒ±œÇ', ' heaven he blessed and having broken'), ('œÖ·º±·ø∑ ŒîŒ±œÖŒØŒ¥ Œï·ΩêŒªŒøŒ≥Œ∑ŒºŒ≠ŒΩŒøœÇ ·ΩÅ ·ºêœÅœáœåŒºŒµŒΩŒøœÇ', ' Son of David blessed [is] he who comes'), ('·ºÇŒΩ Œµ·º¥œÄŒ∑œÑŒµ Œï·ΩêŒªŒøŒ≥Œ∑ŒºŒ≠ŒΩŒøœÇ ·ΩÅ ·ºêœÅœáœåŒºŒµŒΩŒøœÇ', ' anyhow you say Blessed [is] he who comes')]})\n"
                    ]
                },
                {
                    "ename": "IndexError",
                    "evalue": "list index out of range",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-16-2f0a5bbe466a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproduce_material\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"didache\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m<ipython-input-13-49f4370ada93>\u001b[0m in \u001b[0;36mproduce_material\u001b[0;34m(output_file_name, data, blanks)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mcurr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mconcordances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"concordances\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mIndexError\u001b[0m: list index out of range"
                    ]
                }
            ],
            "source": [
                "produce_material(\"didache\", d)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[('BibleHub', 'Œ¥œçŒø', {'concordances': {'Original Word': 'Œ¥œçŒø', 'Part of Speech': 'Adjective; Indeclinable Numeral (Adjective)', 'Transliteration': 'duo', 'Phonetic Spelling': \"(doo'-o)\", 'Definition': 'two', 'Usage': 'two.'}, 'examples': [('Œµ·º∂Œ¥ŒµŒΩ ·ºÑŒªŒªŒøœÖœÇ Œ¥œçŒø ·ºÄŒ¥ŒµŒªœÜŒøœçœÇ ·º∏Œ¨Œ∫œâŒ≤ŒøŒΩ', ' he saw others two brothers James'), (\"ŒºŒµœÑ' Œ±·ΩêœÑŒø·ø¶ Œ¥œçŒø \", ' with him two'), ('Œü·ΩêŒ¥Œµ·Ω∂œÇ Œ¥œçŒΩŒ±œÑŒ±Œπ Œ¥œÖœÉ·Ω∂ Œ∫œÖœÅŒØŒøŒπœÇ Œ¥ŒøœÖŒªŒµœçŒµŒπŒΩ', ' No one is able two masters to serve')]}), ('BibleHub', '·º±ŒºŒ¨œÑŒπœåŒΩ', {'concordances': {'Original Word': '·º±ŒºŒ¨œÑŒπŒøŒΩ, ŒøœÖ, œÑœå', 'Part of Speech': 'Noun, Neuter', 'Transliteration': 'himation', 'Phonetic Spelling': \"(him-at'-ee-on)\", 'Definition': 'an outer garment, a cloak, robe', 'Usage': ' a long flowing outer garment, tunic.'}, 'examples': [('·ºÄŒ≥ŒΩŒ¨œÜŒøœÖ ·ºêœÄ·Ω∂ ·º±ŒºŒ±œÑŒØ·ø≥ œÄŒ±ŒªŒ±Œπ·ø∑ Œ±·º¥œÅŒµŒπ', ' unshrunk on clothing old tears away'), ('·ºÄœÄ·Ω∏ œÑŒø·ø¶ ·º±ŒºŒ±œÑŒØŒøœÖ Œ∫Œ±·Ω∂ œáŒµ·øñœÅŒøŒΩ', ' from the garment and a worse'), ('Œ∫œÅŒ±œÉœÄŒ≠Œ¥ŒøœÖ œÑŒø·ø¶ ·º±ŒºŒ±œÑŒØŒøœÖ Œ±·ΩêœÑŒø·ø¶ ', ' fringe of the clothing of him')]}), ('BibleHub', 'œáŒπœÑ·ø∂ŒΩŒ±', {'concordances': {'Original Word': 'œáŒπœÑœéŒΩ, ·ø∂ŒΩŒøœÇ, ·ΩÅ', 'Part of Speech': 'Noun, Masculine', 'Transliteration': 'chit√≥n', 'Phonetic Spelling': \"(khee-tone')\", 'Definition': 'a tunic', 'Usage': 'a tunic, garment, undergarment.'}, 'examples': [('ŒºŒ∑Œ¥·Ω≤ Œ¥œçŒø œáŒπœÑ·ø∂ŒΩŒ±œÇ ŒºŒ∑Œ¥·Ω≤ ·ΩëœÄŒøŒ¥ŒÆŒºŒ±œÑŒ±', ' nor two tunics nor sandals'), ('·ºêŒΩŒ¥œçœÉŒ∑œÉŒ∏Œµ Œ¥œçŒø œáŒπœÑ·ø∂ŒΩŒ±œÇ ', ' put on two tunics'), ('Œ¥ŒπŒ±œÅœÅŒÆŒæŒ±œÇ œÑŒø·Ω∫œÇ œáŒπœÑ·ø∂ŒΩŒ±œÇ Œ±·ΩêœÑŒø·ø¶ ŒªŒ≠Œ≥ŒµŒπ', ' having torn the garments of him says')]}), ('BibleHub', 'ŒªŒ¨Œ≤·øÉ', {'concordances': {'Original Word': 'ŒªŒ±ŒºŒ≤Œ¨ŒΩœâ', 'Part of Speech': 'Verb', 'Transliteration': 'lamban√≥', 'Phonetic Spelling': \"(lam-ban'-o)\", 'Definition': 'to take, receive', 'Usage': '(a) I receive, get, (b) I take, lay hold of.'}, 'examples': [('·ΩÅ Œ±·º∞œÑ·ø∂ŒΩ ŒªŒ±ŒºŒ≤Œ¨ŒΩŒµŒπ Œ∫Œ±·Ω∂ ·ΩÅ', ' who asks receives and he that'), ('·ºÄœÉŒ∏ŒµŒΩŒµŒØŒ±œÇ ·º°Œº·ø∂ŒΩ ·ºîŒªŒ±Œ≤ŒµŒΩ Œ∫Œ±·Ω∂ œÑ·Ω∞œÇ', ' infirmities of us he took and our'), ('·ºêŒ∫Œ≤Œ¨ŒªŒªŒµœÑŒµ Œ¥œâœÅŒµ·Ω∞ŒΩ ·ºêŒªŒ¨Œ≤ŒµœÑŒµ Œ¥œâœÅŒµ·Ω∞ŒΩ Œ¥œåœÑŒµ', ' cast out freely you received freely give')]}), ('BibleHub', 'Œø·ΩêŒ¥·Ω≤', {'concordances': {'Original Word': 'Œø·ΩêŒ¥Œ≠', 'Part of Speech': 'Conjunction,Negative', 'Transliteration': 'oude', 'Phonetic Spelling': \"(oo-deh')\", 'Definition': 'and not, neither', 'Usage': 'neither, nor, not even, and not.'}, 'examples': [('œÄŒ±œÅŒ±œÄœÑœéŒºŒ±œÑŒ± Œ±·ΩêœÑ·ø∂ŒΩ Œø·ΩêŒ¥·Ω≤ ·ΩÅ œÄŒ±œÑ·Ω¥œÅ', ' trespasses of them neither the Father'), ('Œø·Ωê Œ¥ŒπŒøœÅœçœÉœÉŒøœÖœÉŒπŒΩ Œø·ΩêŒ¥·Ω≤ Œ∫ŒªŒ≠œÄœÑŒøœÖœÉŒπŒΩ ', ' not do break in nor steal'), ('Œø·Ωê œÉœÄŒµŒØœÅŒøœÖœÉŒπŒΩ Œø·ΩêŒ¥·Ω≤ Œ∏ŒµœÅŒØŒ∂ŒøœÖœÉŒπŒΩ Œø·ΩêŒ¥·Ω≤', ' not they sow nor do they reap nor')]}), ('BibleHub', 'Œ†Œ±ŒΩœÑ·Ω∂', {'concordances': {'Original Word': 'œÄ·æ∂œÇ, œÄ·æ∂œÉŒ±, œÄ·æ∂ŒΩ', 'Part of Speech': 'Adjective', 'Transliteration': 'pas', 'Phonetic Spelling': '(pas)', 'Definition': 'all, every', 'Usage': 'all, the whole, every kind of.'}, 'examples': [(\"·ºêœÑŒ±œÅŒ¨œáŒ∏Œ∑ Œ∫Œ±·Ω∂ œÄ·æ∂œÉŒ± ·º∏ŒµœÅŒøœÉœåŒªœÖŒºŒ± ŒºŒµœÑ'\", ' he was troubled and all Jerusalem with'), ('Œ∫Œ±·Ω∂ œÉœÖŒΩŒ±Œ≥Œ±Œ≥·ΩºŒΩ œÄŒ¨ŒΩœÑŒ±œÇ œÑŒø·Ω∫œÇ ·ºÄœÅœáŒπŒµœÅŒµ·øñœÇ', ' And having gathered together all the chief priests'), ('·ºÄœÄŒøœÉœÑŒµŒØŒªŒ±œÇ ·ºÄŒΩŒµ·øñŒªŒµŒΩ œÄŒ¨ŒΩœÑŒ±œÇ œÑŒø·Ω∫œÇ œÄŒ±·øñŒ¥Œ±œÇ', ' having sent forth he put to death all the boys')]})]\n",
                        "[('BibleHub', 'Œ¥œçŒø', {'concordances': {'Original Word': 'Œ¥œçŒø', 'Part of Speech': 'Adjective; Indeclinable Numeral (Adjective)', 'Transliteration': 'duo', 'Phonetic Spelling': \"(doo'-o)\", 'Definition': 'two', 'Usage': 'two.'}, 'examples': [('Œµ·º∂Œ¥ŒµŒΩ ·ºÑŒªŒªŒøœÖœÇ Œ¥œçŒø ·ºÄŒ¥ŒµŒªœÜŒøœçœÇ ·º∏Œ¨Œ∫œâŒ≤ŒøŒΩ', ' he saw others two brothers James'), (\"ŒºŒµœÑ' Œ±·ΩêœÑŒø·ø¶ Œ¥œçŒø \", ' with him two'), ('Œü·ΩêŒ¥Œµ·Ω∂œÇ Œ¥œçŒΩŒ±œÑŒ±Œπ Œ¥œÖœÉ·Ω∂ Œ∫œÖœÅŒØŒøŒπœÇ Œ¥ŒøœÖŒªŒµœçŒµŒπŒΩ', ' No one is able two masters to serve')]}), ('BibleHub', '·º±ŒºŒ¨œÑŒπœåŒΩ', {'concordances': {'Original Word': '·º±ŒºŒ¨œÑŒπŒøŒΩ, ŒøœÖ, œÑœå', 'Part of Speech': 'Noun, Neuter', 'Transliteration': 'himation', 'Phonetic Spelling': \"(him-at'-ee-on)\", 'Definition': 'an outer garment, a cloak, robe', 'Usage': ' a long flowing outer garment, tunic.'}, 'examples': [('·ºÄŒ≥ŒΩŒ¨œÜŒøœÖ ·ºêœÄ·Ω∂ ·º±ŒºŒ±œÑŒØ·ø≥ œÄŒ±ŒªŒ±Œπ·ø∑ Œ±·º¥œÅŒµŒπ', ' unshrunk on clothing old tears away'), ('·ºÄœÄ·Ω∏ œÑŒø·ø¶ ·º±ŒºŒ±œÑŒØŒøœÖ Œ∫Œ±·Ω∂ œáŒµ·øñœÅŒøŒΩ', ' from the garment and a worse'), ('Œ∫œÅŒ±œÉœÄŒ≠Œ¥ŒøœÖ œÑŒø·ø¶ ·º±ŒºŒ±œÑŒØŒøœÖ Œ±·ΩêœÑŒø·ø¶ ', ' fringe of the clothing of him')]}), ('BibleHub', 'œáŒπœÑ·ø∂ŒΩŒ±', {'concordances': {'Original Word': 'œáŒπœÑœéŒΩ, ·ø∂ŒΩŒøœÇ, ·ΩÅ', 'Part of Speech': 'Noun, Masculine', 'Transliteration': 'chit√≥n', 'Phonetic Spelling': \"(khee-tone')\", 'Definition': 'a tunic', 'Usage': 'a tunic, garment, undergarment.'}, 'examples': [('ŒºŒ∑Œ¥·Ω≤ Œ¥œçŒø œáŒπœÑ·ø∂ŒΩŒ±œÇ ŒºŒ∑Œ¥·Ω≤ ·ΩëœÄŒøŒ¥ŒÆŒºŒ±œÑŒ±', ' nor two tunics nor sandals'), ('·ºêŒΩŒ¥œçœÉŒ∑œÉŒ∏Œµ Œ¥œçŒø œáŒπœÑ·ø∂ŒΩŒ±œÇ ', ' put on two tunics'), ('Œ¥ŒπŒ±œÅœÅŒÆŒæŒ±œÇ œÑŒø·Ω∫œÇ œáŒπœÑ·ø∂ŒΩŒ±œÇ Œ±·ΩêœÑŒø·ø¶ ŒªŒ≠Œ≥ŒµŒπ', ' having torn the garments of him says')]}), ('BibleHub', 'ŒªŒ¨Œ≤·øÉ', {'concordances': {'Original Word': 'ŒªŒ±ŒºŒ≤Œ¨ŒΩœâ', 'Part of Speech': 'Verb', 'Transliteration': 'lamban√≥', 'Phonetic Spelling': \"(lam-ban'-o)\", 'Definition': 'to take, receive', 'Usage': '(a) I receive, get, (b) I take, lay hold of.'}, 'examples': [('·ΩÅ Œ±·º∞œÑ·ø∂ŒΩ ŒªŒ±ŒºŒ≤Œ¨ŒΩŒµŒπ Œ∫Œ±·Ω∂ ·ΩÅ', ' who asks receives and he that'), ('·ºÄœÉŒ∏ŒµŒΩŒµŒØŒ±œÇ ·º°Œº·ø∂ŒΩ ·ºîŒªŒ±Œ≤ŒµŒΩ Œ∫Œ±·Ω∂ œÑ·Ω∞œÇ', ' infirmities of us he took and our'), ('·ºêŒ∫Œ≤Œ¨ŒªŒªŒµœÑŒµ Œ¥œâœÅŒµ·Ω∞ŒΩ ·ºêŒªŒ¨Œ≤ŒµœÑŒµ Œ¥œâœÅŒµ·Ω∞ŒΩ Œ¥œåœÑŒµ', ' cast out freely you received freely give')]}), ('BibleHub', 'Œø·ΩêŒ¥·Ω≤', {'concordances': {'Original Word': 'Œø·ΩêŒ¥Œ≠', 'Part of Speech': 'Conjunction,Negative', 'Transliteration': 'oude', 'Phonetic Spelling': \"(oo-deh')\", 'Definition': 'and not, neither', 'Usage': 'neither, nor, not even, and not.'}, 'examples': [('œÄŒ±œÅŒ±œÄœÑœéŒºŒ±œÑŒ± Œ±·ΩêœÑ·ø∂ŒΩ Œø·ΩêŒ¥·Ω≤ ·ΩÅ œÄŒ±œÑ·Ω¥œÅ', ' trespasses of them neither the Father'), ('Œø·Ωê Œ¥ŒπŒøœÅœçœÉœÉŒøœÖœÉŒπŒΩ Œø·ΩêŒ¥·Ω≤ Œ∫ŒªŒ≠œÄœÑŒøœÖœÉŒπŒΩ ', ' not do break in nor steal'), ('Œø·Ωê œÉœÄŒµŒØœÅŒøœÖœÉŒπŒΩ Œø·ΩêŒ¥·Ω≤ Œ∏ŒµœÅŒØŒ∂ŒøœÖœÉŒπŒΩ Œø·ΩêŒ¥·Ω≤', ' not they sow nor do they reap nor')]}), ('BibleHub', 'Œ†Œ±ŒΩœÑ·Ω∂', {'concordances': {'Original Word': 'œÄ·æ∂œÇ, œÄ·æ∂œÉŒ±, œÄ·æ∂ŒΩ', 'Part of Speech': 'Adjective', 'Transliteration': 'pas', 'Phonetic Spelling': '(pas)', 'Definition': 'all, every', 'Usage': 'all, the whole, every kind of.'}, 'examples': [(\"·ºêœÑŒ±œÅŒ¨œáŒ∏Œ∑ Œ∫Œ±·Ω∂ œÄ·æ∂œÉŒ± ·º∏ŒµœÅŒøœÉœåŒªœÖŒºŒ± ŒºŒµœÑ'\", ' he was troubled and all Jerusalem with'), ('Œ∫Œ±·Ω∂ œÉœÖŒΩŒ±Œ≥Œ±Œ≥·ΩºŒΩ œÄŒ¨ŒΩœÑŒ±œÇ œÑŒø·Ω∫œÇ ·ºÄœÅœáŒπŒµœÅŒµ·øñœÇ', ' And having gathered together all the chief priests'), ('·ºÄœÄŒøœÉœÑŒµŒØŒªŒ±œÇ ·ºÄŒΩŒµ·øñŒªŒµŒΩ œÄŒ¨ŒΩœÑŒ±œÇ œÑŒø·Ω∫œÇ œÄŒ±·øñŒ¥Œ±œÇ', ' having sent forth he put to death all the boys')]})]\n",
                        "[]\n",
                        "[]\n"
                    ]
                }
            ],
            "source": [
                "for item in d:\n",
                "    print(item)\n",
                "    print(item)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data = d[0]\n",
                "blanks = d[1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Œ¥œçŒø\n",
                        "·º±ŒºŒ¨œÑŒπœåŒΩ\n",
                        "œáŒπœÑ·ø∂ŒΩŒ±\n",
                        "ŒªŒ¨Œ≤·øÉ\n",
                        "Œø·ΩêŒ¥·Ω≤\n",
                        "Œ†Œ±ŒΩœÑ·Ω∂\n"
                    ]
                }
            ],
            "source": [
                "for item in data:\n",
                "    print(item[1])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Œ¥œçŒø\n",
                        "·º±ŒºŒ¨œÑŒπœåŒΩ\n",
                        "œáŒπœÑ·ø∂ŒΩŒ±\n",
                        "ŒªŒ¨Œ≤·øÉ\n",
                        "Œø·ΩêŒ¥·Ω≤\n",
                        "Œ†Œ±ŒΩœÑ·Ω∂\n"
                    ]
                }
            ],
            "source": [
                "produce_material(\"didache\", data, blanks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['ŒºŒµœÑ', \"'\", 'Œ±·ΩêœÑŒø·ø¶', '<b>Œ¥œçŒø</b>', ';', '·ºê·Ω∞ŒΩ', '·ºÑœÅ·øÉ', 'œÑŒπœÇ', 'œÑ·Ω∏', '·º±ŒºŒ¨œÑŒπœåŒΩ', 'œÉŒøœÖ', ',', 'Œ¥·Ω∏œÇ', 'Œ±·Ωê'] <b>Œ¥œçŒø</b>\n",
                        "['ŒºŒµœÑ', \"'\", 'Œ±·ΩêœÑŒø·ø¶', 'Œ¥œçŒø', ';', '·ºê·Ω∞ŒΩ', '·ºÑœÅ·øÉ', 'œÑŒπœÇ', 'œÑ·Ω∏', '<b>·º±ŒºŒ¨œÑŒπœåŒΩ</b>', 'œÉŒøœÖ', ',', 'Œ¥·Ω∏œÇ', 'Œ±·ΩêœÑ·ø∑', 'Œ∫Œ±·Ω∂', 'œÑ·Ω∏ŒΩ', 'œáŒπœÑ·ø∂ŒΩŒ±', ';', '·ºê·Ω∞ŒΩ'] <b>·º±ŒºŒ¨œÑŒπœåŒΩ</b>\n",
                        "['œÇ', 'œÑ·Ω∏', '·º±ŒºŒ¨œÑŒπœåŒΩ', 'œÉŒøœÖ', ',', 'Œ¥·Ω∏œÇ', 'Œ±·ΩêœÑ·ø∑', 'Œ∫Œ±·Ω∂', 'œÑ·Ω∏ŒΩ', '<b>œáŒπœÑ·ø∂ŒΩŒ±</b>', ';', '·ºê·Ω∞ŒΩ', 'ŒªŒ¨Œ≤·øÉ', 'œÑŒπœÇ', '·ºÄœÄ·Ω∏', 'œÉŒø·ø¶', 'œÑ·Ω∏', 'œÉœåŒΩ', ',', 'Œº·Ω¥'] <b>œáŒπœÑ·ø∂ŒΩŒ±</b>\n",
                        "['œÉŒøœÖ', ',', 'Œ¥·Ω∏œÇ', 'Œ±·ΩêœÑ·ø∑', 'Œ∫Œ±·Ω∂', 'œÑ·Ω∏ŒΩ', 'œáŒπœÑ·ø∂ŒΩŒ±', ';', '·ºê·Ω∞ŒΩ', '<b>ŒªŒ¨Œ≤·øÉ</b>', 'œÑŒπœÇ', '·ºÄœÄ·Ω∏', 'œÉŒø·ø¶', 'œÑ·Ω∏', 'œÉœåŒΩ', ',', 'Œº·Ω¥', '·ºÄœÄŒ±ŒØœÑŒµŒπ', ';', 'Œø·Ωê'] <b>ŒªŒ¨Œ≤·øÉ</b>\n",
                        "['Œ≤·øÉ', 'œÑŒπœÇ', '·ºÄœÄ·Ω∏', 'œÉŒø·ø¶', 'œÑ·Ω∏', 'œÉœåŒΩ', ',', 'Œº·Ω¥', '·ºÄœÄŒ±ŒØœÑŒµŒπ', ';', '<b>Œø·ΩêŒ¥·Ω≤</b>', 'Œ≥·Ω∞œÅ', 'Œ¥œçŒΩŒ±œÉŒ±Œπ', '.', 'Œ†Œ±ŒΩœÑ·Ω∂', 'œÑ·ø∑', 'Œ±·º∞œÑŒø·ø¶ŒΩœÑŒØ', 'œÉŒµ', 'Œ¥'] <b>Œø·ΩêŒ¥·Ω≤</b>\n",
                        "['œåŒΩ', ',', 'Œº·Ω¥', '·ºÄœÄŒ±ŒØœÑŒµŒπ', ';', 'Œø·ΩêŒ¥·Ω≤', 'Œ≥·Ω∞œÅ', 'Œ¥œçŒΩŒ±œÉŒ±Œπ', '.', '<b>Œ†Œ±ŒΩœÑ·Ω∂</b>', 'œÑ·ø∑', 'Œ±·º∞œÑŒø·ø¶ŒΩœÑŒØ', 'œÉŒµ', 'Œ¥ŒØŒ¥ŒøœÖ', 'Œ∫Œ±·Ω∂', 'Œº·Ω¥', '·ºÄœÄŒ±ŒØœÑŒµŒπ'] <b>Œ†Œ±ŒΩœÑ·Ω∂</b>\n"
                    ]
                }
            ],
            "source": [
                "for item in data:\n",
                "    c = wf.concordance_list(cltk_normalize(item[1]))\n",
                "    line, bold = boldify_selected_word(cltk_normalize(item[1]), c[0][6])\n",
                "    print(line, bold)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
        },
        "kernelspec": {
            "display_name": "Python 3.7.3 64-bit",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.3"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
